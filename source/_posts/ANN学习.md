---
title: ANN学习
date: 2018-05-03 10:13:37
tags: 
- 神经网络 
- ANN
- 机器学习
categories:
- 神经网络
---

## 

ANN人工神经网络。

## 参考文献

1. 《机器学习》第4章 Tom M.Mitchell著
2.  [神经网络基础-Poll笔记]( http://www.cnblogs.com/maybe2030/p/5597716.html)
3. [神经网络浅讲：从神经元到深度学习](http://www.cnblogs.com/subconscious/p/5058741.html)

## 神经网络组成

1) 感知层(perceptron)
2) 线性单元(linear unit)
3) sigmoid单元(sigmoid unit)

##  感知器

### 感知器的表征能力

可以把感知器看作是n维实例空间（即点空间）的超平面决策面。
> 仅有两层深度的感知器网络就可以表示所有的布尔函数，在这些网络中输入被送到多个单元，这些单元的输出被输入到第二级，也是最后一级。

### 感知器学习权向量

学习任务是决定一个权向量，它可以使感知器对于给定的训练样例输出正确的1或-1。

- 分类：  

    1. 感知器训练法则

    2. 增量法则（delta法则）

       根据输入的非阈值化线性组合的误差来更新权。  

- 区别

    1. 误差类型：感知器训练法则根据阈值化的感知器==输出的误差==更新权值，delta法则根据输入的非阈值化==线性组合的误差==来更新权。  
    2. 收敛性：感知器训练法则经过有限次的迭代收敛到一个能离线分类训练数据的假设，但条件必须是训练样例线性可分。而增量法则渐进到最小误差假设，可能需要很长时间，但无论训练样例是否线性可分都收敛。



#### 感知器训练法则

- 公式

  $ w_{i} \leftarrow w_{i} + \Delta w_{i}$

  其中： $\Delta w_{i} = \eta(t-o)x_{i}$


- 适用范围：
  1. 训练样例线性可分，如果不可分则无法收敛。
  2. 只适用于单神经元的场合。

#### 梯度下降和delta法则

- 概念

  delta的关键思想是使用==梯度下降(gradient descent)==来搜索可能的权向量的假设空间，以找到最佳拟合训练样例的权向量。

  需要指定一个度量标准来衡量假设（权向量）相对于训练样例的训练误差。常用度量标准：

  $$ E(\overline{w}) \equiv \frac{1}{2} \sum_{d\in D}(t_d - o_d)^2$$

  其中, D为训练用例集合，$t_d$是训练样例d的目标输出。$o_d$是线性单元对训练样例d的输出。

- 梯度下降权值更新法则

  公式：$\Delta w_i = \eta \sum_{d \in D}(t_d-o_d)x_{id} $

  梯度下降适用情况：

  ```
  1)假设空间包含连续参数化的假设。
  2)误差对于这些假设可微。
  ```

  梯度下降存在问题：

  ```
  1) 有时收敛过程非常慢
  2）有时在误差曲面上有多个局部极小值，不能保证这个过程会找到全局最小值。	
  ```

  优化方法：

  ```
  增量梯度下降
  随机梯度下降
  ```

- 适用范围：

  1. 训练样例线性不可分。
  2. 只适用于单层的感知器。

## 多层网络和反向传播算法

![[一文弄懂神经网络中的反向传播法——BackPropagation](http://www.cnblogs.com/charlotte77/p/5629865.html)](https://images2015.cnblogs.com/blog/853467/201606/853467-20160630141449671-1058672778.png)

### 

### Step1 前向传播

需要先对w1、w2...、w8取初始值，为随机数。

#### 1. 输入层—>隐含层

#### 2. 隐含层—>到出层

### Step2 反向传播

#### 1.计算总误差

#### 2.隐含层—>输出层的权值更新

#### 3.隐含层—>隐含层的权值更新